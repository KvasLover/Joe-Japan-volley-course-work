[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/V-2uIfVx)
[![Open in Codespaces](https://classroom.github.com/assets/launch-codespace-2972f46106e565e64193e422d61a12cf1da4916b45550586e14ef0a7c637dd04.svg)](https://classroom.github.com/open-in-codespaces?assignment_repo_id=17999740)
# –û—Ç—á—ë—Ç –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã ‚Ññ1
--------------------------------------------
–í—ã–ø–æ–ª–Ω–∏–ª–∞: –í–∞–ª—å–º—É—Å –£.–ú. –≥—Ä. 210901
–ü—Ä–æ–≤–µ—Ä–∏–ª: –ü—Ä—É–¥–Ω–∏–∫ –ê.–ú.
---------------------------------------------
üìù  –î–ª—è –¥–∞–Ω–Ω–æ–π –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã —è –±—É–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GitHub CodeSpace.
–í –∫–∞—á–µ—Å—Ç–≤–µ –∑–∞–¥–∞–Ω–∏—è —è –≤—ã–±—Ä–∞–ª–∞ —Å–ª–µ–¥—É—é—â–µ–µ: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö.

–î–ª—è —Ä–∞–±–æ—Ç—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã –ø—Ä–∏—à–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å pyspark:
![alt text](pyspark.png)

1 –∑–∞–¥–∞–Ω–∏–µ:

–û—Ç—Ñ–∏–ª—å—Ç—Ä—É–π—Ç–µ —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –≤–æ–∑—Ä–∞—Å—Ç –±–æ–ª—å—à–µ 30 –ª–µ—Ç.

–†–µ—à–µ–Ω–∏–µ:

```
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("DataFiltering").getOrCreate()
data = [("John", 25), ("Alice", 30), ("Bob", 35)]
df = spark.createDataFrame(data, ["Name", "Age"])
filtered_df = df.filter(df["Age"] > 30)
filtered_df.show()
```

–†–µ–∑—É–ª—å—Ç–∞—Ç:
![alt text](task1.png)


2 –∑–∞–¥–∞–Ω–∏–µ:

–ü—Ä–µ–æ–±—Ä–∞–∑—É–π—Ç–µ DataFrame, —á—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü "Age_Group", –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ª—é–¥–µ–π –Ω–∞ –≥—Ä—É–ø–ø—ã "–ú–æ–ª–æ–¥—ã–µ" –∏ "–í–∑—Ä–æ—Å–ª—ã–µ" –ø–æ –≤–æ–∑—Ä–∞—Å—Ç—É (–º–µ–Ω—å—à–µ 30 - "–ú–æ–ª–æ–¥—ã–µ", 30 –∏ –±–æ–ª—å—à–µ - "–í–∑—Ä–æ—Å–ª—ã–µ").

–†–µ—à–µ–Ω–∏–µ:

```
from pyspark.sql.functions import when
df_with_age_group = df.withColumn("Age_Group", when(df["Age"] < 30, "–ú–æ–ª–æ–¥—ã–µ").otherwise("–í–∑—Ä–æ—Å–ª—ã–µ"))
df_with_age_group.show()
```

–†–µ–∑—É–ª—å—Ç–∞—Ç:
![alt text](task2.png)



3 –∑–∞–¥–∞–Ω–∏–µ:

–†–∞—Å—Å—á–∏—Ç–∞–π—Ç–µ —Å—Ä–µ–¥–Ω–∏–π –≤–æ–∑—Ä–∞—Å—Ç –¥–ª—è –∫–∞–∂–¥–æ–π –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π –≥—Ä—É–ø–ø—ã ("–ú–æ–ª–æ–¥—ã–µ" –∏ "–í–∑—Ä–æ—Å–ª—ã–µ").

–†–µ—à–µ–Ω–∏–µ:

```
from pyspark.sql.functions import avg
average_age_df = df_with_age_group.groupBy("Age_Group").agg(avg("Age").alias("Average_Age"))
average_age_df.show()
```

–†–µ–∑—É–ª—å—Ç–∞—Ç:
![alt text](task3.png)

–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ –∞–Ω–∫–µ—Ç—ã –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏.
–í–æ–ø—Ä–æ—Å 1:
–û–±—ä—è—Å–Ω–∏—Ç–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ Hadoop –≤ —ç–∫–æ—Å–∏—Å—Ç–µ–º–µ –ë–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –µ–≥–æ —Å–≤—è–∑—å —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ HDFS.

–û—Ç–≤–µ—Ç: Hadoop - —ç—Ç–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ì–ª–∞–≤–Ω–∞—è —Ü–µ–ª—å Hadoop - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–≥—Ä–æ–º–Ω—ã–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ —Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–º–∏ –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö. Hadoop –∏—Å–ø–æ–ª—å–∑—É–µ—Ç HDFS (Hadoop Distributed File System) –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. HDFS —Ä–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏–µ –±–ª–æ–∫–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏—Ö –ø–æ –∫–ª–∞—Å—Ç–µ—Ä—É, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–∞–∂–µ –≤ —Å–ª—É—á–∞–µ –æ—Ç–∫–∞–∑–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —É–∑–ª–æ–≤.

–í–æ–ø—Ä–æ—Å 2:
–û–ø–∏—à–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã Hadoop –∏ –∏—Ö —Ä–æ–ª–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö.

–û—Ç–≤–µ—Ç: –≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ Hadoop –≤–∫–ª—é—á–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:
HDFS (Hadoop Distributed File System): –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å.
YARN (Yet Another Resource Negotiator): –ú–µ–Ω–µ–¥–∂–µ—Ä —Ä–µ—Å—É—Ä—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ—Å—É—Ä—Å—ã –∫–ª–∞—Å—Ç–µ—Ä–∞ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏.
MapReduce: –ú–æ–¥–µ–ª—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö –ø—É—Ç–µ–º —Ä–∞–∑–±–∏–µ–Ω–∏—è –∑–∞–¥–∞—á –Ω–∞ –ø–æ–¥–∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ.
HBase: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö.
Hive: –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —è–∑—ã–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤, –ø–æ—Ö–æ–∂–µ–≥–æ –Ω–∞ SQL.
Pig: –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤.
Spark: –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –∫–∞–∫ –ø–∞–∫–µ—Ç–Ω—É—é, —Ç–∞–∫ –∏ –ø–æ—Ç–æ–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É.

–í–æ–ø—Ä–æ—Å 3:
–û–±—Å—É–¥–∏—Ç–µ —à–∞–≥–∏ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —Å—Ä–µ–¥—ã Hadoop –∏ Spark, –≤—ã–¥–µ–ª—è—è –≤–∞–∂–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã.

–û—Ç–≤–µ—Ç:
–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Java: Hadoop –∏ Spark —Ç—Ä–µ–±—É—é—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Java. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è JAVA_HOME –∏ PATH –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ.
–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ Hadoop: –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏ —Ä–∞—Å–ø–∞–∫—É–π—Ç–µ –¥–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤ Hadoop. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã core-site.xml, hdfs-site.xml–∏ mapred-site.xml.
–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ HDFS: –í—ã–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è HDFS –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º –∑–∞–ø—É—Å–∫–æ–º.
–ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω–æ–≤ Hadoop: –ó–∞–ø—É—Å—Ç–∏—Ç–µ –¥–µ–º–æ–Ω—ã NameNode, DataNode, ResourceManager –∏ NodeManager.
–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ Spark: –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏ —Ä–∞—Å–ø–∞–∫—É–π—Ç–µ –¥–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤ Spark. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è SPARK_HOME –∏ PATH.
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Spark: –û–±–Ω–æ–≤–∏—Ç–µ —Ñ–∞–π–ª spark-defaults.conf–¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Spark.
–ó–∞–ø—É—Å–∫ Spark: –ó–∞–ø—É—Å—Ç–∏—Ç–µ Spark –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–µ.

–í–æ–ø—Ä–æ—Å 4:
–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –æ–±—â–∏–µ –º–µ—Ç–æ–¥—ã —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ–ø–æ–ª–∞–¥–æ–∫ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ Hadoop –∏ Spark.

–û—Ç–≤–µ—Ç:
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∂—É—Ä–Ω–∞–ª–æ–≤: –ò–∑—É—á–∏—Ç–µ –∂—É—Ä–Ω–∞–ª—ã Hadoop –∏ Spark –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ—à–∏–±–æ–∫.
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø—É—Ç–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.
–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è: –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è JAVA_HOME, HADOOP_HOME –∏ SPARK_HOME –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –≤–µ—Ä–Ω–æ.
–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ç–µ–≤—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫: –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ —É–∑–ª—ã –∫–ª–∞—Å—Ç–µ—Ä–∞ –º–æ–≥—É—Ç –æ–±–º–µ–Ω–∏–≤–∞—Ç—å—Å—è –¥–∞–Ω–Ω—ã–º–∏ –∏ –∏–º–µ—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–µ—Ç–∏.
–ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ —Å–ª—É–∂–±: –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –¥–µ–º–æ–Ω—ã Hadoop –∏ Spark –ø–æ—Å–ª–µ –≤–Ω–µ—Å–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é.

–í–æ–ø—Ä–æ—Å 5:
–û–±—ä—è—Å–Ω–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ Hadoop –∏ Spark –∏ –∫–∞–∫ —ç—Ç–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
–û—Ç–≤–µ—Ç: –õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (data locality) –∏–≥—Ä–∞–µ—Ç –≤–∞–∂–Ω—É—é —Ä–æ–ª—å –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ Hadoop –∏ Spark. –ü—Ä–∏–Ω—Ü–∏–ø –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–∞ –∫–∞–∫ –º–æ–∂–Ω–æ –±–ª–∏–∂–µ –∫ –º–µ—Å—Ç—É –∏—Ö —Ö—Ä–∞–Ω–µ–Ω–∏—è. –≠—Ç–æ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–µ—Ç–∏, —á—Ç–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∑–∞–¥–µ—Ä–∂–∫–∏ –∏ –ø–æ–≤—ã—à–∞–µ—Ç –æ–±—â—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã. –í Hadoop –∏ Spark —É–∑–ª—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç –∑–∞–¥–∞—á–∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞ —Ç–µ—Ö —É–∑–ª–∞—Ö, –≥–¥–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ—Å—É—Ä—Å—ã –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏ —É—Å–∫–æ—Ä—è–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á.
---

## üéØ Lab 1: Hadoop and Spark - Big Data Environment Setup

### üìö Objective
This lab aims to establish a Big Data environment using Hadoop and Spark, fundamental tools in the Big Data ecosystem. Students will learn to set up these tools, configure them, and execute basic operations to ensure a functional environment for future data processing tasks.

### üìù Tasks

#### üìå Task 1: Joining the Classroom
- Join the classroom using the invitation link provided by your instructor after creating your GitHub account.
- Follow the instructions to join the classroom, granting access to course materials and assignment submissions.

#### üìå Task 2: Joining the Course
- Once in the classroom, join the specific course related to this lab.
- Navigate to the course page within the classroom interface and follow the instructions to join.
- This step provides access to the course syllabus, schedule, and assignments.

#### üìå Task 3: Setting Up the Environment
- Install and configure Hadoop and Spark on your local machine or use GitHub Codespaces for development.
- Ensure your machine meets the necessary system requirements for Hadoop and Spark.
- If using GitHub Codespaces:
  - Launch a virtual development environment in your browser by opening the repository and selecting "Open with Codespaces."
- For local setup:
  - Download Hadoop and Spark from their official pages.
  - Follow the installation instructions provided by the documentation.
- Verify correct installation by running basic commands:
  - `hadoop version`: Checks the installed version of Hadoop.
  - `spark-shell`: Launches the Spark shell for Scala.
  - `pyspark`: Launches the Spark shell for Python.
  - `spark-shell --master local[2]`: Launches Spark shell with two local worker threads.

#### üìå Task 4: Software Installation and Configuration
- **Downloading Required Software:**
  - Download the appropriate versions of Hadoop and Spark compatible with your system and course requirements.
  - Obtain Hadoop from the [Apache Hadoop official page](https://hadoop.apache.org/releases.html) and Spark from the [Apache Spark official page](https://spark.apache.org/downloads.html).
- **Configuring the Software:**
  - After downloading, extract the software files.
  - Configure Hadoop and Spark to work in your environment:
    - Edit XML configuration files located in the `etc/hadoop` and `conf` directories for Hadoop and Spark, respectively.
    - Adjust configurations based on your specific setup and course requirements.
- **Verifying Installation:**
  - Confirm correct installation by running basic commands:
    - `hadoop version`: Checks the installed version of Hadoop.
    - `hadoop fs -ls /`: Lists the contents of the root directory in Hadoop's distributed file system.

#### üìå Task 5: Running Simple Spark Programs
- Write and execute Spark programs to validate setup and execution in your environment. Explore various use cases for Spark programs, such as:
  - **Data Transformation:** Perform data transformations like filtering, mapping, and aggregating on large datasets.
  - **Machine Learning:** Implement machine learning algorithms for tasks like classification, regression, and clustering.
  - **Graph Processing:** Analyze graph data structures for applications such as social network analysis and recommendation systems.
  - **Real-time Streaming:** Process real-time data streams from sources like sensors, logs, or social media feeds.
  - **Text Analysis:** Conduct text mining and natural language processing tasks like sentiment analysis, topic modeling, and entity recognition.
  - **Geospatial Analysis:** Analyze spatial data for applications in GIS, location-based services, and urban planning.
  - **Image Processing:** Apply image processing techniques for tasks like object detection, image classification, and image segmentation.
  - **Time Series Analysis:** Analyze time series data for forecasting, anomaly detection, and trend analysis.
  - **Joining Datasets:** Perform join operations on multiple datasets to combine and analyze related information.

Two examples are provided below:

  - **Simple Data Analysis Program (Python):**
    ```python
    from pyspark.sql import SparkSession
    
    # Create SparkSession
    spark = SparkSession.builder.appName("SimpleDataAnalysis").getOrCreate()
    
    # Create DataFrame
    data = [("John", 25), ("Alice", 30), ("Bob", 35)]
    df = spark.createDataFrame(data, ["Name", "Age"])
    
    # Perform transformations and actions
    df.show()
    df.filter(df["Age"] > 30).show()
    ```

  - **Simple Data Analysis Program (Scala):**
    ```scala
    import org.apache.spark.sql.SparkSession
    
    // Create SparkSession
    val spark = SparkSession.builder().appName("SimpleDataAnalysis").getOrCreate()
    
    // Create DataFrame
    import spark.implicits._
    val data = Seq(("John", 25), ("Alice", 30), ("Bob", 35))
    val df = data.toDF("Name", "Age")
    
    // Perform transformations and actions
    df.show()
    df.filter($"Age" > 30).show()
    ```

  - **Pi Estimation Program (Python):**
    ```python
    from pyspark.sql import SparkSession
    import random
    
    # Create SparkSession
    spark = SparkSession.builder.appName("PiEstimation").getOrCreate()
    
    # Number of points to generate
    num_points = 1000000
    
    # Generate random points and count those inside the unit circle
    inside_circle = spark.sparkContext.parallelize(range(0, num_points)) \
                    .filter(lambda _: random.random()**2 + random.random()**2 < 1).count()
    
    # Estimate Pi
    pi_estimate = 4 * inside_circle / num_points
    print("Estimated Pi:", pi_estimate)
    ```

  - **Pi Estimation Program (Scala):**
    ```scala
    import org.apache.spark.sql.SparkSession
    
    // Create SparkSession
    val spark = SparkSession.builder().appName("PiEstimation").getOrCreate()
    
    // Number of points to generate
    val num_points = 1000000
    
    // Generate random points and count those inside the unit circle
    val inside_circle = spark.sparkContext.parallelize(0 until num_points)
                        .filter { _ =>
                          val x = math.random()
                          val y = math.random()
                          x * x + y * y < 1
                        }.count()
    
    // Estimate Pi
    val pi_estimate = 4.0 * inside_circle / num_points
    println(s"Estimated Pi: $pi_estimate")
    ```

- Submit your programs as `.py` or `.scala` files, depending on the language used.

---

## üß† Self-Check Questionnaire

### üîç Question 1:
Explain the purpose of Hadoop in the Big Data ecosystem and its relationship with distributed storage systems like HDFS.

### üîç Question 2:
Describe the key components of the Hadoop ecosystem and their respective roles in data processing and analysis.

### üîç Question 3:
Discuss the steps involved in setting up a Hadoop and Spark environment, highlighting important configuration parameters and potential challenges.

### üîç Question 4:
Identify common troubleshooting techniques for resolving issues during Hadoop and Spark installation and configuration.

### üîç Question 5:
Explain the significance of data locality in Hadoop and Spark processing and how it contributes to performance optimization.
